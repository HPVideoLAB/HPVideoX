export default {
  meau: {
    nav1: "Modèles",
    nav2: "Mineurs",
    nav3: "Apprentissage",
    nav4: "Créateurs",
    nav5: "Blog",
    nav6: "Académie",
    nav7: "Centre d’aide",
    nav8: "Comment utiliser HPVideo",
    nav9: "Aller sur HPVideo"
  },
  home: {
    cont1: {
      title1: "HPVideo",
      title2: "Plateforme décentralisée de génération vidéo par IA sur la blockchain BNB",
      title3: "HPVideo est une plateforme décentralisée de génération vidéo par IA, fonctionnant avec un portefeuille numérique et basée sur la blockchain BNB. Elle permet une création vidéo rapide et économique grâce à de multiples modèles d'IA, sans nécessiter d'adresse e-mail ni de données personnelles",
      text1: "Vitesse ×10",
      text2: "coût",
      btn: "Commencer à créer",
      model1: {
        text: "Caractéristiques : modèle de référence du secteur pour sa compréhension profonde du monde physique et ses capacités de simulation. Il génère des vidéos de niveau cinématographique avec des scènes complexes, des personnages cohérents et des changements de caméra multi-angles, tout en respectant les lois physiques réelles.",
        tips: "Exemple : une immense mammouth laineux émerge lentement d’une forêt arctique enveloppée de brouillard ; ses longs poils se balancent naturellement, sa respiration forme des nuages blancs au coucher du soleil, tandis que la caméra se déplace en douceur pour créer une tension cinématographique."
      },
      model2: {
        text: "Caractéristiques : spécialisé dans la génération de vidéos longues et cohérentes (jusqu’à une minute et plus). Excellente constance des personnages, du style et des décors sur toute la durée, idéal pour des courts métrages complets avec début, développement et fin.",
        tips: "Exemple : un plan continu de 60 secondes suivant un astronaute solitaire qui fait son premier pas sur une planète extraterrestre colorée. Gros plan sur sa botte écrasant une mousse lumineuse, puis remontée vers son visage émerveillé derrière la visière, avant de dévoiler un paysage fluorescent baigné par deux lunes."
      },
      model3: {
        text: "Caractéristiques : réputé pour son moteur physique avancé et sa génération vidéo HDR de haute qualité. Particulièrement performant pour simuler les mouvements complexes des objets, la dynamique des fluides (eau, fumée) et l’interaction réaliste de la lumière et des ombres.",
        tips: "Exemple : une vidéo au ralenti d’un ballon d’eau qui explose contre un mur en plein soleil. On voit le moment exact de l’impact, la forme parfaite de la bulle, puis des milliers de gouttes scintillantes, ainsi que les variations de lumière sur la surface du mur."
      },
      model4: {
        text: "Caractéristiques : équilibre réussi entre réalisme, détails et fluidité du mouvement. Les vidéos générées présentent des expressions faciales crédibles, des textures fines (peau, tissu, métal) et des décors riches, avec des animations naturelles et continues.",
        tips: "Exemple : gros plan sur un artisan soufflant du verre en fusion. La caméra met en avant le flux du verre rougeoyant, la lueur de la chaleur, la texture des outils et l’expression concentrée du visage de l’artisan."
      },
      model5: {
        text: "Caractéristiques : suite de création vidéo IA intégrée, avec un fort degré de contrôle utilisateur. Paramètres détaillés pour l’animation de la caméra et la cohérence des personnages, idéal pour des vidéos narratives avec des mouvements fluides et des émotions marquées.",
        tips: "Exemple : une scène de polar dans une bibliothèque sombre. Un détective ouvre un vieux livre poussiéreux ; la caméra capte son expression méfiante, les particules de poussière dans le faisceau lumineux et une carte cachée entre les pages, le tout sous une lumière dramatique."
      },
      model6: {
        text: "Caractéristiques : génère des vidéos dans différents formats (du vertical pour les réseaux sociaux au grand écran cinéma) et accorde une grande importance à la synchronisation audio-vidéo, pour une expérience plus immersive.",
        tips: "Exemple : une courte vidéo verticale 9:16 de style cyberpunk. Un DJ mixe sur un toit illuminé de néons ; les mouvements de la foule et les effets lumineux se synchronisent parfaitement avec une piste électronique originale."
      },
      model7: {
        text: "Caractéristiques : met l’accent sur le rendu réaliste de scènes très dynamiques. Excelle pour les phénomènes à grande échelle (tempêtes, vagues, batailles, trafic urbain dense, foules) avec des effets de lumière et de physique crédibles.",
        tips: "Exemple : un plan aérien sur un champ de bataille médiéval au crépuscule. Des milliers de soldats s’affrontent, une pluie de flèches traverse le ciel, des colonnes de fumée montent des feux de camp, et des bannières claquent violemment dans le vent."
      },
      model8: {
        text: "Caractéristiques : spécialisé dans la narration multi-plans avec des transitions stables et fluides. Pensé pour générer automatiquement des vidéos courtes composées de gros plans, plans moyens et plans larges, parfait pour des clips publicitaires ou des mini-histoires.",
        tips: "Exemple : une séquence de 30 secondes façon cinéma muet sur une lettre perdue et retrouvée : 1) gros plan sur une vieille lettre dans une boîte aux lettres ; 2) plan moyen d’un facteur à vélo sous la pluie ; 3) transition douce vers un gros plan du destinataire, souriant en ouvrant la lettre."
      }
    },
    cont2: {
      title: "Que puis-je vous aider à créer ?",
      text1: "Une plume blanche flotte et tourbillonne entre des nuages d’orage zébrés d’éclairs, avant d’être happée par un vortex de vent, laissant dans le ciel une trajectoire paisible au milieu du chaos.",
      text2: "Une goutte d’encre noire tombe dans une eau limpide, se diffuse lentement et se mêle en volutes, comme une nébuleuse naissant dans un verre, révélant un océan abstrait en constante mutation.",
      text3: "Un petit bateau en papier, plié dans une vieille carte, dérive dans le ruissellement de la pluie. L’eau reflète les néons de la ville, transformant le caniveau en rivière de lumière.",
      text4: "Une grue en origami se déploie d’elle-même sur une table en bois baignée de soleil, la feuille se lissant peu à peu avant de redevenir un simple rectangle, couvert de plis comme autant de souvenirs.",
      text5: "Le reflet d’une ville futuriste scintille dans une flaque d’eau. Lorsqu’une goutte tombe, des ondulations déchirent et recomposent le paysage urbain, évoquant une peinture abstraite en mouvement."
    },
    cont3: {
      title1: "Noyau intelligent décentralisé",
      text1_1: "Puissance de calcul distribuée",
      text1_2: "Intégration IA profonde",
      text1_3: "Génération vérifiable",
      text1_4: "Création efficace",
      title2: "Du texte aux images en mouvement",
      text2_1: "Entrée texte",
      text2_2: "Construction image par image",
      text2_3: "Génération vidéo",
      text2_4: "Sortie haute qualité",
      title3: "Votre création, réellement à vous",
      text3_1: "Horodatage de la création",
      text3_2: "Processus transparent",
      text3_3: "Valeur protégée",
      text3_4: "Propriété garantie",
      title4: "Co-création ouverte",
      text4_1: "Inspiration mondiale connectée",
      text4_2: "Récits partagés",
      text4_3: "Construction collaborative",
      text4_4: "Histoires innovantes"
    },
    cont4: {
      title: "Génération de vidéo à partir de texte"
    },
    cont5: {
      title: "Cas d’usage fréquents",
      text1: "Transformation et montage vidéo",
      text2: "Moodboards visuels",
      text3: "Prévisualisation d’espace",
      text4: "Capture de performance",
      text5: "Essayage virtuel",
      text6: "Exploration de design",
      text7: "Storyboard",
      text8: "Animatique / pré-production",
      text9: "Générer votre propre image",
      text10: "Effets visuels",
      text_cont1: {
        title: "Monter, transformer et générer des vidéos",
        text: "En décrivant simplement ce que vous voulez en langage naturel, vous pouvez modifier l’éclairage, la composition, les sujets ou ajouter/supprimer des éléments, sans pipeline de post-production complexe."
      },
      text_cont2: {
        title: "Créer des moodboards sur mesure",
        text: "À partir d’une seule image de référence, développez un moodboard complet pour un événement ou un projet, en générant d’autres images et vidéos dans le même style visuel."
      },
      text_cont3: {
        title: "Mettre en scène un espace en quelques secondes",
        text: "Sans 3D ni compositing, fournissez une photo de la pièce et décrivez les objets, HPVideo génère une mise en scène visuelle complète."
      },
      text_cont4: {
        title: "Une capture de mouvement pour tous",
        text: "Plus besoin de rigs, d’animation manuelle ou de studio de motion capture. Filmez simplement une performance avec n’importe quelle caméra, puis transférez ce mouvement sur le personnage de référence."
      },
      text_cont5: {
        title: "Essayer toutes les tenues que vous voulez",
        text: "Avec une photo de la personne et des images de vêtements, vous pouvez tester d’innombrables looks, sans tournage supplémentaire ni détourage."
      },
      text_cont6: {
        title: "Trouver l’inspiration par l’image",
        text: "Utilisez des objets trouvés, des textures et des photos comme point de départ pour explorer de nouveaux concepts en architecture, design et plus."
      },
      text_cont7: {
        title: "Transformer des prompts en storyboard",
        text: "En combinant un style unique et une série de prompts textuels, générez en quelques minutes un storyboard complet au lieu d’y passer plusieurs jours."
      },
      text_cont8: {
        title: "Du storyboard à l’animatique",
        text: "Grâce à la conversion image-vers-vidéo, passez rapidement de l’idée à une animatique haute fidélité sans coûts et délais élevés."
      },
      text_cont9: {
        title: "Générer différentes versions de vous-même",
        text: "Avec un selfie, créez d’innombrables variations de vous-même dans divers décors, tenues et styles."
      },
      text_cont10: {
        title: "Créer des effets visuels complexes",
        text: "Avec le modèle Runway Aleph, vous pouvez générer des effets visuels avancés simplement en les décrivant."
      }
    },
    cont6: {
      title: "Vitesse ×10, 10 % du coût",
      cost_cont1: {
        title: "Conçu pour la vitesse",
        text1: "Architecture de modèle allégée",
        text2: "Calcul massivement parallèle",
        text3: "Coût de calcul réduit",
        text4: "Pipeline d’inférence à haut débit"
      },
      cost_cont2: {
        title: "Accélération matérielle",
        text1: "Rendu optimisé GPU",
        text2: "Matériel dédié à l’IA",
        text3: "Gestion mémoire efficace",
        text4: "Sortie à faible latence"
      },
      cost_cont3: {
        title: "10× plus rapide, 10 % du prix",
        text1: "Structure de modèle optimisée",
        text2: "Efficacité du calcul distribué",
        text3: "Pipeline de traitement parallèle",
        text4: "Rendu accéléré par matériel"
      },
      cost_cont4: {
        title: "Maximiser la valeur, minimiser le coût",
        text1: "Utilisation optimisée des ressources",
        text2: "Modèle extensible et économique",
        text3: "Coût d’exploitation réduit",
        text4: "Performance-prix améliorée"
      }
    },
    cont7: {
      title1: "Reconnu et",
      title2: "soutenu par l’écosystème"
    },
    cont8: {
      title: "Notre communauté est une force sans limites"
    },
    model: {
      title: "Modèles pris en charge par HPVideo",
      learn_more: "En savoir plus",
      li1: {
        title: "Wan2.5",
        text1: "Date de sortie : 24 septembre 2025 (aperçu). La version « WAN 2.5 » est disponible en API et accès public.",
        text2: "Développeur : Alibaba Tongyi Lab (laboratoire d’IA d’Alibaba Cloud)."
      },
      li2: {
        title: "Ovi",
        text1: "Date de sortie : 15 octobre 2025 (aperçu public). La version open source a été publiée le 23 octobre 2025.",
        text2: "Développeur : Character AI (laboratoire d’IA basé aux États-Unis), en collaboration avec l’université de Yale."
      },
      li3: {
        title: "VEO 3.1",
        text1: "Date de sortie : 8 août 2025 (version test pour entreprises). L’aperçu public est progressivement ouvert depuis le 1er novembre 2025.",
        text2: "Développeur : Google DeepMind (laboratoire d’IA au Royaume-Uni, filiale d’Alphabet)."
      },
      li4: {
        title: "LTX2 Pro",
        text1: "Date de sortie : 20 juillet 2025 (test exclusif mobile). L’API Web a été lancée le 30 octobre 2025.",
        text2: "Développeur : Lightricks (entreprise israélienne spécialisée dans les outils créatifs IA)."
      },
      li5: {
        title: "HAILUO 02",
        text1: "Date de sortie : 30 septembre 2025 (test fermé pour partenaires). L’API publique et la démo Web sont sorties le 10 novembre 2025.",
        text2: "Développeur : Minimax (start-up chinoise axée sur la génération multimodale et les solutions IA pour entreprises)."
      },
      li6: {
        title: "SEEDSNCE V1",
        text1: "Date de sortie : 12 octobre 2025 (test interne écosystème ByteDance). L’API publique et l’intégration à TikTok Creator Studio ont été lancées le 20 novembre 2025.",
        text2: "Développeur : ByteDance AI Lab (division R&D IA d’une entreprise technologique mondiale)."
      },
      li7: {
        title: "KLING V2.0",
        text1: "Date de sortie : 5 novembre 2025 (test public développeurs). L’API entreprise et les options d’auto-hébergement sont disponibles depuis le 22 novembre 2025.",
        text2: "Développeur : Kwaivgi (start-up IA basée à Singapour, dédiée à l’écosystème développeur global)."
      },
      li8: {
        title: "PIXVERSE V4.5",
        text1: "Date de sortie : 18 septembre 2025 (test public pour créateurs). L’API entreprise et les fonctions collaboratives ont été lancées le 15 novembre 2025.",
        text2: "Développeur : Pixverse AI (start-up américaine avec une équipe R&D mondiale, axée sur la génération visuelle de haute qualité)."
      },
      li9: {
        title: "SORA (OpenAI)",
        text1: "Date de sortie : février 2024 (aperçu). La version « SORA 2 » n’a pas encore été annoncée officiellement.",
        text2: "Développeur : OpenAI (laboratoire de recherche en IA basé aux États-Unis)."
      }
    }
  },
  models: {
    title: "HPVideo",
    text1: "HPVideo est une plateforme pionnière de génération vidéo IA décentralisée, qui s’appuie sur un réseau distribué pour offrir une puissance de création fiable et efficace. Les utilisateurs peuvent transformer des prompts texte en images animées de haute qualité, construites image par image.",
    text2: "HPVideo vise également à créer un environnement de co-création ouvert, connectant la communauté mondiale afin de bâtir ensemble des histoires innovantes et des récits partagés.",
    model1: {
      title: "Wan2.5",
      text1: "Date de sortie : 24 septembre 2025 (aperçu). La version « WAN 2.5 » est disponible en API et accès public.",
      text2: "Développeur : Alibaba Tongyi Lab (laboratoire d’IA d’Alibaba Cloud).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos avec son (voix, effets, musique) à partir de texte ou d’image, jusqu’à 10 secondes.",
      text3_3: "Architecture multimodale native permettant des instructions complexes (simulation physique, mouvements de caméra) et des récits cohérents.",
      text3_4: "Compatible avec de nombreux scénarios (paysages urbains, actions de personnages, phénomènes naturels) et prise en charge de la création multimodale (édition d’images, texte-vers-image).",
      text4: "Sortie : vidéos haute résolution (480p–1080p, 24 fps), forte cohérence temporelle, détails riches et rendu HDR."
    },
    model2: {
      title: "Ovi",
      text1: "Date de sortie : 15 octobre 2025 (aperçu public). Version open source publiée le 23 octobre 2025.",
      text2: "Développeur : Character AI (laboratoire d’IA basé aux États-Unis), en collaboration avec Yale.",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos de 5 secondes avec son à partir de texte seul ou de texte + image, avec mouvements naturels, éclairage réaliste et profondeur de champ.",
      text3_3: "Architecture bi-colonne (11 milliards de paramètres, dont 5 milliards pour l’audio) offrant une synchronisation audio-vidéo fluide et des interactions physiques crédibles.",
      text3_4: "Prise en charge de plusieurs formats (9:16, 16:9, 1:1) et scénarios variés (actions humaines, scènes de vie). Version open source auto-hébergeable.",
      text4: "Sortie : clips vidéo 720×720 (24 fps), jusqu’à 1080p pour les options avancées, avec voix ou effets sonores synchronisés."
    },
    model3: {
      title: "VEO 3.1",
      text1: "Date de sortie : 8 août 2025 (test entreprise). Aperçu public progressif à partir du 1er novembre 2025.",
      text2: "Développeur : Google DeepMind (laboratoire d’IA britannique, filiale d’Alphabet).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos haute fidélité de 15 secondes à partir de prompts texte, ajustables via des itérations multiples.",
      text3_3: "Basé sur le modèle multimodal Gemini Ultra 2, avec simulations physiques cohérentes (météo, mouvement des tissus) et continuité spatiale pour les longs plans.",
      text3_4: "Intégration à Google Workspace (export direct vers Slides/Meet) et possibilités de changement de style (ciné, anime, réaliste) en temps réel.",
      text4: "Sortie : vidéos 1080p (30 fps), HDR 8 bits, forte richesse des détails, sons d’ambiance synchronisés (voix off personnalisable)."
    },
    model4: {
      title: "LTX2 Pro",
      text1: "Date de sortie : 20 juillet 2025 (test mobile exclusif). API Web disponible depuis le 30 octobre 2025.",
      text2: "Développeur : Lightricks (entreprise israélienne, connue pour ses outils créatifs IA).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos de 8 secondes en moins de 20 secondes à partir de texte ou d’image, avec optimisation pour mobiles et réseaux sociaux.",
      text3_3: "Modèle léger (2 milliards de paramètres), avec presets pour Reels/TikTok, filtres rétro et autocollants animés.",
      text3_4: "Export en un clic vers Instagram/TikTok (recadrage automatique en 9:16) et synchronisation audio-vidéo (bibliothèque musicale intégrée).",
      text4: "Sortie : vidéos 720p (30 fps), transitions fluides, couleurs optimisées pour les réseaux sociaux, avec segment audio court synchronisé."
    },
    model5: {
      title: "HAILUO 02",
      text1: "Date de sortie : 30 septembre 2025 (test fermé pour partenaires). API publique et démo Web disponibles depuis le 10 novembre 2025.",
      text2: "Développeur : Minimax (start-up chinoise spécialisée dans la génération multimodale et les solutions IA pour entreprises).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos de 12 secondes à partir de texte, image ou combinaison des deux, avec adaptation renforcée au contexte linguistique et culturel chinois.",
      text3_3: "Architecture cloud-edge hybride (8 milliards de paramètres), avec temps d’inférence inférieur à 15 secondes par vidéo, support des templates personnalisés (marketing, formation, etc.).",
      text3_4: "Intégration avec Douyin, WeChat et autres plateformes ; sous-titres auto-générés (mandarin, cantonais, anglais) et clonage vocal conforme aux réglementations.",
      text4: "Sortie : vidéos 1080p (24 fps), flou de mouvement de niveau cinéma, éclairage réaliste, BGM, voix off et sons d’ambiance synchronisés (formats MP4/WEBM)."
    },
    model6: {
      title: "SEEDSNCE V1",
      text1: "Date de sortie : 12 octobre 2025 (test interne ByteDance). API REST publique et intégration à TikTok Creator Studio depuis le 20 novembre 2025.",
      text2: "Développeur : ByteDance AI Lab (division R&D IA d’un groupe technologique global).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos courtes de 10 secondes à partir de prompts texte, optimisée pour les formats TikTok (défis de danse, démonstrations produits).",
      text3_3: "Basé sur le framework LightMultimodal 3.0 (4 milliards de paramètres), offrant une inférence ultra-rapide (moins de 12 secondes) et une faible latence d’API.",
      text3_4: "Intégration profonde avec TikTok/Douyin (export en un clic, recommandations automatiques de hashtags), styles préconfigurés (filtres tendance, effets de texte, transitions dynamiques) conformes aux politiques globales.",
      text3_5: "Sortie par défaut en 480p, avec option d’upscaling vers 720p via une API interne.",
      text4: "Sortie : vidéos 480p (30 fps, formats 9:16 et 16:9), mouvements fluides, couleurs optimisées TikTok, audio court (musique, voix, effets) synchronisé (formats MP4/AVC)."
    },
    model7: {
      title: "KLING V2.0",
      text1: "Date de sortie : 5 novembre 2025 (test public développeur). API entreprise et options d’auto-hébergement disponibles depuis le 22 novembre 2025.",
      text2: "Développeur : Kwaivgi (start-up IA basée à Singapour, axée sur la génération multimodale pour les développeurs).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos de 14 secondes à partir de texte, image ou combinaison, avec prise en charge native de plus de 20 langues de prompts.",
      text3_3: "Architecture Transformer modulaire (6 milliards de paramètres), temps d’inférence inférieur à 18 secondes, ajustements sur mesure pour des cas d’usage verticaux (e-commerce, éducation, etc.).",
      text3_4: "Déploiement flexible : API cloud, installation locale et intégration edge (ARM/x86). Filtrage de contenu intégré, conforme au RGPD/CCPA.",
      text3_5: "Fonctions d’édition avancées : ajustement image par image, contrôle de vitesse, remplacement de piste audio (compatible bibliothèques libres de droits).",
      text4: "Sortie : vidéos 1080p (24/30 fps, formats 1:1, 9:16, 16:9), rendu très réaliste, cohérence spatiale forte, audio synchronisé (voix off, sons d’ambiance, pistes personnalisées) en MP4/WEBM/AV1."
    },
    model8: {
      title: "PIXVERSE V4.5",
      text1: "Date de sortie : 18 septembre 2025 (test public pour créateurs). API entreprise et fonctions de collaboration disponibles depuis le 15 novembre 2025 (feuille de route pour la V5.0 et la 4K prévue T1 2026).",
      text2: "Développeur : Pixverse AI (start-up américaine avec équipe R&D mondiale, spécialisée dans la génération visuelle haut de gamme).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos cinématographiques de 16 secondes à partir de texte, images ou storyboards, avec contrôle avancé du style (réaliste, anime, cyberpunk, aquarelle, etc., plus de 30 presets).",
      text3_3: "Transformer multimodal de 12 milliards de paramètres, avec cohérence temporelle renforcée (moins de scintillement) et rendu physique (fluides, tissus, réfraction lumineuse).",
      text3_4: "Génération par lot (jusqu’à 50 vidéos par appel API), extension intelligente de scène (prolongation cohérente de la vidéo) ; intégrations avec Adobe Creative Cloud (plugins Premiere Pro/After Effects) et Figma.",
      text3_5: "Post-production assistée par IA : correction colorimétrique, stabilisation, génération automatique de sous-titres (plus de 40 langues, traduction contextuelle pour audiences globales).",
      text4: "Sortie : vidéos 1080p (24/30/60 fps, formats 1:1, 9:16, 16:9, 21:9), HDR 10 bits, plage dynamique ciné, audio haute fidélité (5.1 pour les clients entreprise), formats MP4/ProRes/AV1 (ProRes réservé à l’édition pro)."
    },
    model9: {
      title: "SORA (OpenAI)",
      text1: "Date de sortie : février 2024 (aperçu). Aucune annonce officielle de « SORA 2 » pour l’instant.",
      text2: "Développeur : OpenAI (laboratoire d’IA américain).",
      text3_1: "Points clés :",
      text3_2: "Génération de vidéos jusqu’à 60 secondes et plus à partir de prompts texte.",
      text3_3: "Simulation physique réaliste (fluides, interactions d’objets) et support de narrations longues.",
      text3_4: "Couvre un large éventail de scènes (villes, comportements humains, phénomènes naturels, etc.).",
      text4: "Sortie : vidéos jusqu’en 1080p, forte cohérence temporelle, textures détaillées."
    }
  },
  miners: {
    text1: "Pour participer au minage GPU, vous devez posséder un nœud NFT. Les récompenses de minage commenceront à être distribuées environ 3 à 6 mois après la mise en ligne sur DEX.",
    text2: "5 milliards de jetons HPC sont émis chaque année pour les récompenses de minage. 10 % sont débloqués immédiatement, le reste étant libéré de manière linéaire sur 180 jours.",
    text3: "Chaque machine connectée au réseau reçoit une récompense HPC pour chaque requête texte-vers-vidéo terminée, mais la vidéo doit être générée dans les 20 blocs (environ 60 secondes), sinon la requête est considérée comme invalide.",
    btn1: "Acheter du HPC",
    btn2: "Échanger HPC sur Uniswap",
    title: "Configuration recommandée",
    title1: "GPU",
    text1_1: "Au moins 1 carte graphique Nvidia (série GeForce 30/40 recommandée). Plus la VRAM est élevée, plus le rendu est rapide et la rentabilité importante.",
    title2: "Mémoire",
    text2_1: "Au minimum 16 Go de RAM recommandés",
    title3: "Stockage",
    text3_1: "Au moins 100 Go d’espace disque libre recommandés",
    title4: "Bande passante",
    text4_1: "Au minimum 5 Mb de bande passante recommandée"
  },
  learn: {
    title: "Votre IA, capable de tout créer",
    text: "Parcourez des tutoriels, des AMA et des analyses approfondies sans cesse enrichis, et intégrez facilement HPVideo dans votre workflow créatif. Que vous débutiez ou que vous souhaitiez maîtriser des techniques avancées, HPVideo Academy est votre point de départ.",
    text1: "Catégories",
    text2: "Tout"
  },
  help: {
    title: "Présentation de SORA",
    text: "Notre nouvelle génération de modèles IA pour la génération média cohérente avec le monde réel.",
    btn1: "Essayer maintenant",
    btn2: "Consulter la documentation",
    title1: "Toutes les ressources dont vous avez besoin pour créer",
    text1: "Commencez avec ces thèmes rapides pour apprendre à utiliser les outils de génération vidéo et d’IA créative de HPVideo.",
    desc1: {
      title: "Compte et facturation",
      text: "Gérez votre abonnement et vos informations de facturation",
      btn1: "Gérer l’abonnement",
      btn2: "Gérer le compte",
      btn3: "Crédits et quotas",
      btn4: "Factures"
    },
    desc2: {
      title: "Créer avec HPVideo",
      text: "Apprenez à utiliser les outils et fonctionnalités de HPVideo pour mener vos projets à bien",
      btn1: "Gen-4",
      btn2: "Workflows",
      btn3: "Démarrer",
      btn4: "Plus d’outils"
    },
    desc3: {
      title: "Entreprise",
      text: "Informations pour la gestion de comptes entreprise",
      btn1: "Ressources admin",
      btn2: "Ressources membre"
    },
    desc4: {
      title: "Assets et espaces de travail",
      text: "Organisez et gérez vos fichiers et espaces collaboratifs",
      btn1: "Gestion des assets",
      btn2: "Types de fichiers pris en charge",
      btn3: "Espaces de travail"
    },
    desc5: {
      title: "Dépannage",
      text: "Trouvez des solutions aux problèmes techniques courants et aux messages d’erreur",
      btn1: "Gérer l’abonnement",
      btn2: "Dépannage de la plateforme",
      btn3: "Obtenir de l’aide technique"
    },
    desc6: {
      title: "Communauté, confidentialité et contenu",
      text: "Découvrez les programmes communautaires HPVideo et les politiques d’usage et de contenu",
      btn1: "HPVideo Éducation",
      btn2: "Communauté",
      btn3: "Confiance et sécurité"
    }
  },
  faqs: {
    title: "Questions fréquentes",
    text: "FAQ",
    qs1: {
      title: "1. Qu’est-ce que HPVideo ?",
      text: "HPVideo est une plateforme décentralisée de génération vidéo IA basée sur BNB Chain, conçue comme une couche d’infrastructure vidéo pour l’ère des agents IA. Les utilisateurs connectent leur wallet, choisissent un modèle (SORA, WAN, KLING, etc.), saisissent un prompt et une durée, paient une petite somme en USDT, puis reçoivent une vidéo de haute qualité – sans e-mail, sans compte ni données personnelles."
    },
    qs2: {
      title: "2. Comment HPVideo génère-t-il les vidéos ?",
      text: "HPVideo s’appuie sur un réseau de calcul décentralisé. Les requêtes vidéo, les paiements et les horodatages sont enregistrés sur BNB Chain, et les nœuds répartis sur le réseau effectuent le rendu des vidéos."
    },
    qs3: {
      title: "3. Pourquoi utiliser la connexion par wallet ?",
      text: "HPVideo ne demande ni e-mail ni compte centralisé. L’authentification par wallet protège mieux la vie privée, renforce la sécurité et simplifie l’accès pour les utilisateurs Web2 comme Web3."
    },
    qs4: {
      title: "4. En quoi HPVideo est-il décentralisé ?",
      text: "Les requêtes, paiements et horodatages sont inscrits sur BNB Chain, et le rendu est effectué par des nœuds de calcul distribués. Cela garantit transparence, traçabilité et propriété vérifiable du contenu généré."
    },
    qs5: {
      title: "5. Quels modèles IA sont pris en charge ?",
      text: "HPVideo prend en charge WAN 2.5, SORA, OVI, VEO 3.1, LTX2 Pro, KLING V2.0, PIXVERSE V4.5 et d’autres modèles. Chaque modèle est adapté à des styles visuels et des cas d’usage différents."
    },
    qs6: {
      title: "6. Combien coûte la génération d’une vidéo ?",
      text: "Le coût dépend du modèle et de la durée. La plupart des vidéos coûtent entre 0,3 et 1 USDT, payés via le wallet sur BNB Chain."
    },
    qs7: {
      title: "7. Qui possède les vidéos générées ?",
      text: "Les vidéos appartiennent au wallet qui les a générées. L’horodatage on-chain constitue une preuve vérifiable d’originalité et de propriété."
    },
    qs8: {
      title: "8. Les créateurs Web2 peuvent-ils l’utiliser ?",
      text: "Oui. Le processus est simple : connecter un wallet → entrer un prompt → générer la vidéo. Aucune connaissance crypto n’est nécessaire."
    },
    qs9: {
      title: "9. HPVideo a-t-il un jeton ?",
      text: "Oui. HPVideo émettra le jeton $HPC, utilisé pour la gouvernance, les incitations, la participation au calcul et diverses utilités dans l’écosystème. Les détails seront précisés dans le livre blanc officiel."
    },
    qs10: {
      title: "10. Quel est l’objectif à long terme de HPVideo ?",
      text: "HPVideo vise à devenir la couche d’infrastructure vidéo décentralisée pour les créateurs, les agents IA et les applications Web3 du monde entier, en offrant un processus transparent, une propriété vérifiable et un écosystème multi-modèles."
    }
  },
  footer: {
    title: "Liens utiles",
    meau1: "Github",
    meau2: "Mineurs",
    meau3: "Livre blanc",
    meau4: "FAQ",
    meau5: "$HPC",
    title1: "Commencer avec HPVideo",
    btn1: "Essayer HPVideo",
    btn2: "Voir la démo",
    footer_bottom: "2025 HPVideo. Tous droits réservés."
  }
}