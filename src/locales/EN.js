export default {
  meau: {
    nav1: "Models",
    nav2: "Miners",
    nav3: "Learn",
    nav4: "Creator",
    nav5: "Blog",
    nav6: "Academy",
    nav7: "Help Center",
    nav8: "Ways to use HPVideo",
    nav9: "Go to HPVideo"
  },
  home: {
    cont1: {
      title1: "HPVideo",
      title2: "Decentralized AI Video Generation Platform on BNB Chain",
      title3: "HPVideo is a wallet-based decentralized AI video generation platform on BNB Chain, offering fast, low-cost video creation with multiple AI models—no email, no personal data required",
      text1: "10X faster",
      text2: "the Cost",
      btn: "Create",
      model1: {
        text: "Features and Advantages: As an industry benchmark, it is renowned for its deep understanding and simulation capabilities of the physical world. It can generate cinematic videos with complex scenes, coherent characters, and multi-angle shot transitions, especially excelling in maintaining the movement of objects in accordance with realistic physical laws.",
        tips: "A cinematic trailer of a majestic woolly mammoth emerging from a misty, snowy forest in the Arctic, its long fur swaying realistically as it breathes out clouds of steam in the golden hour light, shot with a dynamic camera movement."
      },
      model2: {
        text: "Features and Advantages: Focused on generating high-quality, visually coherent long videos (potentially one minute or longer). It excels in maintaining consistency in characters, style, and setting throughout the video length, suitable for telling short stories with a beginning, development, climax, and resolution.",
        tips: "A continuous 60-second clip following a lone astronaut’s first steps on a vibrant alien planet. The video starts with a close-up of their boot touching the glowing moss, pans up to show their awestruck face reflected in the helmet, and ends with a wide shot of a breathtaking bioluminescent landscape under twin moons."
      },
      model3: {
        text: "Features and Advantages: Known for its advanced physics engine and high-quality, high dynamic range video generation. Particularly adept at simulating complex object motion, fluid dynamics (such as water and smoke), and realistic light and shadow interactions, generating lifelike and dynamic scenes.",
        tips: "A slow-motion video of a water balloon bursting against a concrete wall in sharp sunlight, capturing every detail: the moment of impact, the perfect spherical shape of the water before it explodes into thousands of shimmering droplets, and the shadows dancing on the wall."
      },
      model4: {
        text: "Features and Advantages: Achieves a good balance between dynamic performance and image detail. The generated videos not only have smooth and natural movements, but also are very realistic in character expressions, material textures (such as skin, cloth, and metal), and scene details, with excellent visual quality.",
        tips: "A hyper-detailed close-up of a skilled artisan’s hands sculpting a piece of molten glass. The video focuses on the fluid motion of the glass, the glow of the heat, the texture of the tools, and the intense concentration on the artisan’s face, all in smooth motion."
      },
      model5: {
        text: "Features and Advantages: As an integrated AI video creation suite, its advantages lie in user control and realistic details. It provides more refined control parameters (such as camera movement, character consistency) and excels at generating narrative videos with smooth character movements and vivid expressions",
        tips: "A suspenseful short film scene where a detective slowly opens an old, dusty book in a library. The camera focuses on the detective’s cautious expression, the flying dust particles in the shaft of light, and the revealing of a hidden map inside the book, with dramatic lighting."
      },
      model6: {
        text: "Features and Advantages: It can flexibly generate videos at different resolutions (such as from social media vertical screens to cinema widescreen), and emphasizes high-quality synchronous generation of audio (such as background music and sound effects) and visuals, providing a more immersive experience.",
        tips: "A vertical-format, 9:16 social media clip of a cyberpunk DJ performing in a neon-drenched rooftop club. The video syncs the pulsating visual effects and crowd movements perfectly with the beat of an original electronic soundtrack."
      },
      model7: {
        text: "Features and Advantages: Emphasizes 'realistic rendering of dynamic scenes,' particularly adept at handling grand, dynamic scenes, such as large-scale natural phenomena (storms, waves), battlefields, busy city traffic, or crowd simulations, and can render them with realistic lighting and physical effects.",
        tips: "A sweeping aerial shot over a medieval battlefield at dusk, with thousands of soldiers clashing. The scene includes realistic dynamics of charging cavalry, flying arrows, billowing smoke from fires, and banners flapping violently in the wind, all rendered with cinematic scale."
      },
      model8: {
        text: 'Features and Advantages: Emphasizing "multi-shot narrative with stable and smooth visuals." Designed to automatically generate short video narratives containing multiple shots (such as close-ups, medium shots, and wide shots), with smooth and stable transitions between shots, it is ideal for quickly creating storyboard-like short films or commercials.',
        tips: "A 30-second silent film-style narrative about a lost letter being returned. The sequence includes: 1) a close-up of an old letter in a mailbox, 2) a medium shot of a postman riding a bicycle through a rainy street, 3) a smooth transition to a close-up of a recipient’s surprised and joyful face."
      }
    },
    cont2: {
      title: "What can I help with?",
      text1: "A single white feather drifts and dances between flashes of lightning in stormy clouds, finally swirling into the rolling vortex, leaving a trail of serene grace against the chaotic sky.",
      text2: "Dense black ink droplets fall into clear water, blooming and intertwining like cosmic nebulae, unfolding into an ever-shifting abstract deep sea within a tranquil glass.",
      text3: "A small paper boat folded from an old map sails bravely down a rain gutter flowing with neon light, navigating a miniature river that mirrors the city's glowing reflections.",
      text4: "An elegant origami crane unfolds itself on a sunlit wooden table, the paper smoothing out with a magical rhythm, eventually returning to a flat sheet filled with memory-bearing creases.",
      text5: "The dazzling reflection of an entire futuristic city shimmers in a rain puddle, rippling and fracturing into a fluid abstract painting as a single drop distorts the liquid mirror.",
    },
    cont3: {
      title1: "Decentralized Intelligent Core",
      text1_1: "Distributed power",
      text1_2: "AI integration",
      text1_3: "Trusted generation",
      text1_4: "Efficient creation",
      title2: "From Text to Motion",
      text2_1: "Text Input",
      text2_2: "Frame-by-frame building",
      text2_3: "Visual generation",
      text2_4: "Quality output",
      title3: "Creativity Authentically Owned",
      text3_1: "Timestamped ideas",
      text3_2: "Transparent process",
      text3_3: "Protected value",
      text3_4: "Guaranteed ownership",
      title4: "Open Collective Creation",
      text4_1: "Global wisdom",
      text4_2: "Shared narratives",
      text4_3: "Collective building",
      text4_4: "Innovative stories",
    },
    cont4: {
      title: "Text to video"
    },
    cont5: {
      title: "Common Use cases",
      text1: "Transform Video",
      text2: "Mood Boards",
      text3: "Visual Staging",
      text4: "Character Performance",
      text5: "Virtual Try-On",
      text6: "Design Exploration",
      text7: "Story Boarding",
      text8: "Animatics",
      text9: "Generate Yourself",
      text10: "Visual Effect",
      text_cont1: {
        title: "Edit, Transform and Generate Video",
        text: "Easily change the lighting of a scene, restyle a shot or subject, add or remove elements from a take and much more. All just by telling the model what you're looking for.",
      },
      text_cont2: {
        title: "Generate Bespoke Mood Boards",
        text: "Turn a single image into a campaign mood board. Simply upload a reference image and instantly begin generating new images and videos in the same style, location and tone.",
      },
      text_cont3: {
        title: "Stage Any Space in Seconds",
        text: "Dress a space without any complex 3D workflows or compositing. Simply provide a reference image of your room alongside the items you’d like to see in it.",
      },
      text_cont4: {
        title: "Motion Capture Anyone Can Use",
        text: "No rigging, animation or expensive mo-cap shoots. Simply record a driving performance using any camera you have, then transpose it directly to a reference character with Act-Two.",
      },
      text_cont5: {
        title: "Try On Anything",
        text: "With References you can restyle a subject in endless outfits. All you need is a single image of your talent and images of your outfits. No additional photoshoots or compositing required.",
      },
      text_cont6: {
        title: "Ideate with Images",
        text: "Find inspiration in found objects, textures and references to explore new concepts and directions for architecture, design and more.",
      },
      text_cont7: {
        title: "Turn Text Prompts into Storyboards",
        text: "Build out entire storyboards in minutes versus days using a single style reference and text prompts.",
      },
      text_cont8: {
        title: "Turn Storyboards into Animatics",
        text: "Bring your static frames to life without the need for costly, time-consuming animation. With Image to Video, you can go from idea to high fidelity proof of concept in a fraction of the time.",
      },
      text_cont9: {
        title: "Generate Yourself",
        text: "Turn a single selfie into endless new images of you across different scenes, wardrobes and locations.",
      },
      text_cont10: {
        title: "Visual Effects",
        text: "With Runway Aleph you can generate complex visual effects just by saying what you want to see.",
      }
    },
    cont6: {
      title: "10×  Speed, 10% Cost",
      cost_cont1: {
        title: "Optimized For Speed",
        text1: "Streamlined model architecture",
        text2: "Massively parallel processing",
        text3: "Minimal computational overhead",
        text4: "High-throughput pipelines",
      },
      cost_cont2: {
        title: "Hardware Accelerated",
        text1: "GPU-optimized rendering",
        text2: "AI-specific hardware",
        text3: "Efficient memory management",
        text4: "Low-latency output",
      },
      cost_cont3: {
        title: "10X Velocity, 10% Cost",
        text1: "Optimized model architecture",
        text2: "Distributed computing efficiency",
        text3: "Parallel processing pipeline",
        text4: "Hardware-accelerated rendering",
      },
      cost_cont4: {
        title: "Maximum Value, Minimum Cost",
        text1: "Optimized resource utilization",
        text2: "Economical scaling model",
        text3: "Lower operational expenses",
        text4: "Enhanced cost-performance ratio",
      }
    },
    cont7: {
      title1: "We’re backed",
      title2: "& Supported by"
    },
    cont8: {
      title: "Our Community is a boundless powerhouse"
    },
    model: {
      title: "HPVideos Supported models",
      learn_more: "Learn more",
      li1: {
        title: "Wan2.5",
        text1: "Release: September 24, 2025 (preview version); the 'WAN 2.5 official version' has opened API access and public experience (no official preview for subsequent versions).",
        text2: "Developer: Alibaba (China) Tongyi Lab (AI R&D institution under Alibaba Cloud Intelligence Group)."
      },
      li2: {
        title: "Ovi",
        text1: "Release: October 15, 2025 (public preview); open-source version launched on October 23, 2025 (no official 'OVI 2' announcement as of November 2025).",
        text2: "Developer: Character AI (U.S.-based AI lab) in collaboration with Yale University’s research team."
      },
      li3: {
        title: "VEO 3.1",
        text1: "Release: August 8, 2025 (beta access for enterprise users); public preview rolled out on November 1, 2025 (no 'Veo4' official announcement as of now).",
        text2: "Developer: Google DeepMind (U.K.-based AI research lab, part of Alphabet Inc.)."
      },
      li4: {
        title: "LTX2 Pro",
        text1: "Release: July 20, 2025 (mobile app exclusive beta); web-based API access launched October 30, 2025 (no 'LTX-3' official timeline as of November 2025).",
        text2: "Developer: Lightricks (Israel-based AI creative tools company, known for Facetune/CapCut competitors)."
      },
      li5: {
        title: "HAILUO 02",
        text1: "Release: September 30, 2025 (closed beta for enterprise partners); public API and web demo launched November 10, 2025 (no official 'Hailuo-03' roadmap as of now).",
        text2: "Developer: Minimax (China-based AI startup, specializing in multimodal generation and enterprise AI solutions)."
      },
      li6: {
        title: "SEEDSNCE V1",
        text1: "Release: October 12, 2025 (internal beta for ByteDance ecosystem apps); public REST API and TikTok Creator Studio integration launched November 20, 2025 (no 'V2' official announcement as of now).",
        text2: "Developer: ByteDance AI Lab (China-based global tech company’s R&D division, specializing in multimodal generation and short-form content tools)."
      },
      li7: {
        title: "KLING V2.0",
        text1: "Release: November 5, 2025 (public beta for developers); enterprise API access and self-hosted deployment options launched November 22, 2025 (no 'V3.0' official roadmap as of now).",
        text2: "Developer: Kwaivgi (Singapore-based AI startup, focused on lightweight multimodal generation for global developer ecosystems)."
      },
      li8: {
        title: "PIXVERSE V4.5",
        text1: "Release: September 18, 2025 (public beta for individual creators); enterprise API and team collaboration features launched November 15, 2025 (official 'V5.0' roadmap teased for Q1 2026, focusing on 4K output).",
        text2: "Developer: Pixverse AI (U.S.-based startup with global R&D teams, specializing in high-quality visual generation for creators and enterprises)"
      },
      li9: {
        title: "SORA (OpenAI)",
        text1: "Release: February 2024 (preview); 'SORA 2' not officially announced (likely speculative versioning).",
        text2: "Developer: OpenAI (U.S.-based AI research lab).",
      }
    }
  },
  models: {
    title: "HPVideo",
    text1: "HPVideo is a pioneering decentralized platform for AI-powered video generation. It utilizes a distributed network to ensure powerful, trustworthy, and efficient creation. Users can easily transform text prompts into high-quality, frame-by-frame visual motion. The platform guarantees authentic ownership, protecting your creativity with timestamped verification and a transparent process.",
    text2: "Furthermore, HPVideo fosters open collective creation, uniting global communities to build upon shared narratives and co-create innovative stories, redefining collaborative digital storytelling.",
    model1: {
      title: "Wan2.5",
      text1: "Release: September 24, 2025 (preview version); the 'WAN 2.5 official version' has opened API access and public experience (no official preview for subsequent versions).",
      text2: "Developer: Alibaba (China) Tongyi Lab (AI R&D institution under Alibaba Cloud Intelligence Group).",
      text3_1: "Features:",
      text3_2: "Generates audio-synced videos (including voice, sound effects, background music) from text/images, with duration extended to 10 seconds.",
      text3_3: "Uses a native multimodal architecture, supporting complex instructions like physics-based simulation (e.g., object interactions) and camera movement, enabling coherent storytelling.",
      text3_4: "Covers diverse scenarios (urban landscapes, human actions, natural phenomena, etc.), and supports full-modal creation (image editing, text-to-image, etc.).",
      text4: "Output: High-resolution video (480p-1080p, 24fps) with temporal consistency and detailed textures, supporting HDR rendering and cinematic color grading."
    },
    model2: {
      title: "Ovi",
      text1: "Release: October 15, 2025 (public preview); open-source version launched on October 23, 2025 (no official 'OVI 2' announcement as of November 2025).",
      text2: "Developer: Character AI (U.S.-based AI lab) in collaboration with Yale University’s research team.",
      text3_1: "Features:",
      text3_2: "Generates 5-second, audio-synced videos from text-only or text+image inputs, with natural motion, lighting, and depth.",
      text3_3: "Uses a twin-backbone cross-modal fusion architecture (11B parameters total, including a 5B voice branch) for physics-aligned interactions and seamless audio-video synchronization.",
      text3_4: "Supports diverse aspect ratios (9:16, 16:9, 1:1) and scenarios (e.g., character actions, daily scenes); open-source for self-hosted deployment.",
      text4: "Output: 720×720 (24fps) video clips (professional-grade options up to 1080p), with temporal consistency and realistic textures; includes synchronized voice/sound effects."
    },
    model3: {
      title: "VEO 3.1",
      text1: "Release: August 8, 2025 (beta access for enterprise users); public preview rolled out on November 1, 2025 (no 'Veo4' official announcement as of now).",
      text2: "Developer: Google DeepMind (U.K.-based AI research lab, part of Alphabet Inc.).",
      text3_1: "Features:",
      text3_2: "Generates 15-second, high-fidelity videos from text prompts (supports multi-turn prompt refinement for scene adjustments).",
      text3_3: "Leverages Gemini Ultra 2’s multimodal foundation for physics-based simulation (e.g., weather dynamics, fabric movement) and long-shot spatial consistency.",
      text3_4: "Integrates with Google Workspace (e.g., direct export to Slides/Meet) and supports real-time style customization (cinematic, anime, photorealistic).",
      text4: "Output: 1080p video (30fps) with 8-bit HDR color grading, detailed object textures, and synchronized ambient audio (supports custom voiceover uploads)."
    },
    model4: {
      title: "LTX2 Pro",
      text1: "Release: July 20, 2025 (mobile app exclusive beta); web-based API access launched October 30, 2025 (no 'LTX-3' official timeline as of November 2025).",
      text2: "Developer: Lightricks (Israel-based AI creative tools company, known for Facetune/CapCut competitors).",
      text3_1: "Features:",
      text3_2: "Generates 8-second, mobile-optimized videos from text/images in <20 seconds (focus on speed for social media creators).",
      text3_3: "Uses a lightweight edge-compatible model (2B parameters) with pre-built style presets (Reels/TikTok trends, vintage filters, animated stickers).",
      text3_4: "Supports direct export to Instagram/TikTok (auto-crops to 9:16) and one-tap audio sync (licensed music library integration).",
      text4: "Output: 720p video (30fps) with smooth motion transitions, social-media-ready color grading, and synchronized short-form audio clips."
    },
    model5: {
      title: "HAILUO 02",
      text1: "Release: September 30, 2025 (closed beta for enterprise partners); public API and web demo launched November 10, 2025 (no official 'Hailuo-03' roadmap as of now).",
      text2: "Developer: Minimax (China-based AI startup, specializing in multimodal generation and enterprise AI solutions).",
      text3_1: "Features:",
      text3_2: "Generates 12-second, high-coherence videos from text, images, or text-image hybrid prompts, with enhanced cultural context adaptation for Chinese scenarios.",
      text3_3: "Adopts a hybrid cloud-edge architecture (8B parameters) for fast inference (<15s per video) and supports custom template imports (e.g., marketing clips, educational content).",
      text3_4: "Integrates with Chinese mainstream platforms (Douyin, WeChat Video 号) for direct export; offers one-click subtitle generation (Mandarin/Cantonese/English) and voice cloning (compliant with data privacy regulations).",
      text4: "Output: 1080p video (24fps) with cinematic motion blur, realistic lighting effects, and synchronized audio (background music, voiceovers, ambient sounds); supports MP4/WEBM formats for cross-platform compatibility."
    },
    model6: {
      title: "SEEDSNCE V1",
      text1: "Release: October 12, 2025 (internal beta for ByteDance ecosystem apps); public REST API and TikTok Creator Studio integration launched November 20, 2025 (no 'V2' official announcement as of now).",
      text2: "Developer: ByteDance AI Lab (China-based global tech company’s R&D division, specializing in multimodal generation and short-form content tools).",
      text3_1: "Features:",
      text3_2: "Generates 10-second, short-form optimized videos from text prompts (supports TikTok-style trend adaptation, e.g., dance challenges, product showcases).",
      text3_3: "Built on ByteDance’s LightMultimodal 3.0 framework (4B parameters) for ultra-fast inference (<12s per video) and low-latency API responses (critical for creator workflows).",
      text3_4: "Native integration with TikTok/ Douyin (one-tap export, hashtag auto-suggestion) and supports style customization (viral filters, text overlays, dynamic transitions); compliant with global content moderation policies.",
      text3_5: "Focus on 480p high-efficiency output (balanced quality/speed) with optional upscaling to 720p via ByteDance’s built-in super-resolution API.",
      text4: "Output: 480p video (30fps, 9:16/16:9 aspect ratios) with smooth motion, TikTok-optimized color grading, and synchronized short-form audio (licensed music library, voiceovers, trend sounds); supports MP4/AVC formats for social media compatibility."
    },
    model7: {
      title: "KLING V2.0",
      text1: "Release: November 5, 2025 (public beta for developers); enterprise API access and self-hosted deployment options launched November 22, 2025 (no 'V3.0' official roadmap as of now).",
      text2: "Developer: Kwaivgi (Singapore-based AI startup, focused on lightweight multimodal generation for global developer ecosystems).",
      text3_1: "Features:",
      text3_2: "Generates 14-second, high-fidelity videos from text, image, or text-image hybrid prompts, with cross-cultural scene adaptation (supports 20+ language prompts natively).",
      text3_3: "Built on a modular transformer architecture (6B parameters) with optimized inference (≤18s per video) and supports custom model fine-tuning (for vertical scenarios like e-commerce product demos, educational content).",
      text3_4: "Offers flexible deployment options: cloud REST API, on-premises self-hosting, and edge-device integration (compatible with ARM/x86 architectures); includes built-in content safety filters (compliant with GDPR/CCPA).",
      text3_5: "Supports advanced editing features: frame-by-frame adjustment, motion speed control, and audio track replacement (compatible with royalty-free music libraries).",
      text4: "Output: 1080p video (24/30fps, 1:1/9:16/16:9 aspect ratios) with realistic texture rendering, spatial consistency, and synchronized audio (voiceovers, ambient sounds, custom audio uploads); supports MP4/WEBM/AV1 formats for cross-platform and low-bandwidth use cases." 
    },
    model8: {
      title: "PIXVERSE V4.5",
      text1: "Release: September 18, 2025 (public beta for individual creators); enterprise API and team collaboration features launched November 15, 2025 (official 'V5.0' roadmap teased for Q1 2026, focusing on 4K output).",
      text2: "Developer: Pixverse AI (U.S.-based startup with global R&D teams, specializing in high-quality visual generation for creators and enterprises).",
      text3_1: "Features:",
      text3_2: "Generates 16-second, cinematic-grade videos from text, image, or storyboard inputs, with advanced style control (photorealistic, anime, cyberpunk, watercolor, etc.—supports 30+ pre-built styles and custom style uploads).",
      text3_3: "Built on a 12B-parameter multimodal transformer with enhanced temporal coherence (reduces frame flicker) and physics-based rendering (e.g., realistic liquid flow, fabric movement, light refraction).",
      text3_4: "Supports batch video generation (up to 50 clips per API call) and intelligent scene extension (auto-expands video length while maintaining narrative consistency); integrates with Adobe Creative Cloud (Premiere Pro/After Effects plugins) and Figma.",
      text3_5: "Offers AI-powered post-editing: one-click color grading, motion stabilization, and subtitle generation (supports 40+ languages with contextual translation for global content).",
      text4: "Output: 1080p video (24/30/60fps, 1:1/9:16/16:9/21:9 aspect ratios) with 10-bit HDR, film-like dynamic range, and synchronized high-fidelity audio (5.1 surround sound support for enterprise users); supports MP4/ProRes/AV1 formats (ProRes exclusive to enterprise plans)." 
    },
    model9: {
      title: "SORA (OpenAI)",
      text1: "Release: February 2024 (preview); 'SORA 2' not officially announced (likely speculative versioning).",
      text2: "Developer: OpenAI (U.S.-based AI research lab).",
      text3_1: "Features:",
      text3_2: "Generates hyper-realistic 60-second+ videos from text prompts.",
      text3_3: "Employs physics-based simulation (e.g., fluid dynamics, object interactions) and coherent long-form storytelling.",
      text3_4: "Supports diverse scenes (urban landscapes, human actions, natural phenomena).",
      text4: "Output: High-resolution video (up to 1080p) with temporal consistency and detailed textures."
    },
    // model10: {
    //   title: "PIXVERSE V4.5 (Pixverse Technologies)",
    //   text1: "Release: V4.5 announced in 2023; ongoing updates.",
    //   text2: "Developer: Pixverse (Singapore-based AI video startup).",
    //   text3_1: "Features:",
    //   text3_2: "Text-to-video with emphasis on dynamic fluidity and realistic details (e.g., facial expressions, material textures).",
    //   text3_3: "Supports multi-scene transitions and custom aspect ratios.",
    //   text3_4: "Optimized for commercial use cases (advertising, content creation).",
    // },
    // model11: {
    //   title: "VEO 3.1 (Hypothetical/Industry-Aligned)",
    //   text1: "Release: Likely post-2023 (based on versioning trends).",
    //   text2: "Developer: Unverified (possible reference to enterprise-grade video tools like HPVideo or Stability AI).",
    //   text3_1: "Features:",
    //   text3_2: "Specializes in long-form video generation (5+ minutes) with minimal temporal artifacts.",
    //   text3_3: "Focuses on visual coherence across shot transitions and complex scene evolution.",
    //   text3_4: "Integrates with text/image inputs for storyboard-aligned outputs.",
    // },
    // model12: {
    //   title: "KLING V2.0 (Physics-Focused Tool)",
    //   text1: "Release: Circa 2023 (speculative).",
    //   text2: "Developer: Likely a research team or studio specializing in physics simulation (e.g., NVIDIA, Unity).",
    //   text3_1: "Features:",
    //   text3_2: "Prioritizes accurate physics-based motion (e.g., rigid body dynamics, cloth simulation).",
    //   text3_3: "Used for technical visualization, gaming, or engineering simulations.",
    //   text3_4: "Combines text prompts with physics parameters for controlled outputs.",
    // },
    // model13: {
    //   title: "LTX 2 PRO (Animation/VFX Tool)",
    //   text1: "Release: Unclear; 'PRO' suggests a premium version of an existing tool.",
    //   text2: "Developer: Possibly a 3D animation studio or AI startup (e.g., D-ID, Synthesia for avatar-focused tools).",
    //   text3_1: "Features:",
    //   text3_2: "Realistic detail rendering (textures, lighting) and smooth character movements.",
    //   text3_3: "Targets virtual production, animated content, or avatar-driven videos.",
    //   text3_4: "May support real-time rendering for interactive applications",
    // },
  },
  miners: {
    text1: "To participate in GPU mining, you need to hold an NFT node.Mining rewards starts 3-6 months after listing on the DEX.",
    text2: "5 billion HPC tokens are mined annually. 10% of the mining rewards unlock immediately, and the rest follow a 180 day linear unlocking schedule.",
    text3: "Machines connected to the network can earn HPC rewards for each successful text-to-video request, but the video generation must be completed within 20 blocks (approximately 60 seconds); otherwise, it is invalid.",
    btn1: "Buy HPC",
    btn2: "Uniswap Exchange HPC",
    title: "Mining Rig Configuration Requirements",
    title1: "GPU",
    text1_1: "Minimum of 1 card, must be an Nvidia graphics card; Nvidia GeForce series recommended, such as the 30 and 40 series. The larger the memory capacity, the faster the rendering speed and the higher the earnings.",
    title2: "Memory",
    text2_1: "It is recommended to have no less than 16GB of RAM",
    title3: "Hard Drive",
    text3_1: "It is recommended to have no less than 100GB of free disk space",
    title4: "Network Bandwidth",
    text4_1: "A network bandwidth of at least 5Mb is recommended",
  },
  learn: {
    title: "Your AI to create anything",
    text: "Explore our expanding library of tutorials, AMAs, and deep-dives to seamlessly integrate HPVideo into your creative and production workflows. Whether you're starting with the basics or advancing to master complex techniques, if you have questions, HPVideo Academy is the perfect place to find answers",
    text1: "Categories",
    text2: "All"
  },
  help: {
    title: "Introducing SORA",
    text: "Our next-generation series of AI models for media generation and world consistency.",
    btn1: "Try now",
    btn2: "See Docs",
    title1: "All the resources you need to make anything you want",
    text1: "Jump right into learning how to use HPVideo's generative video and creative AI tools with these quickstart topics",
    desc1: {
      title: "Account & Billing",
      text: "Manage your subscription and billing information",
      btn1: "Subscription Management",
      btn2: "Account Management",
      btn3: "Credits",
      btn4: "Invoices"
    },
    desc2: {
      title: "Creating with HPVideo",
      text: "Learn how to use HPVideo's tools and features to create your projects",
      btn1: "Gen-4",
      btn2: "Workflows",
      btn3: "Getting Started",
      btn4: "More Tools"
    },
    desc3: {
      title: "Enterprise",
      text: "Information for business customers about Enterprise account management",
      btn1: "Admin Resources",
      btn2: "Member Resources",
    },
    desc4: {
      title: "Assets & Workspaces",
      text: "Organize and manage your files and collaborative workspaces",
      btn1: "Assets",
      btn2: "Supported File Types",
      btn3: "Workspaces",
    },
    desc5: {
      title: "Troubleshooting",
      text: "Find solutions to common technical issues and error messages",
      btn1: "Subscription Management",
      btn2: "Platform Troubleshooting",
      btn3: "Getting Technical Help",
    },
    desc6: {
      title: "Community, Privacy, & Content Policies",
      text: "Information about HPVideo programs alongside guidelines for platform use and content standards",
      btn1: "HPVideo for Educators",
      btn2: "Community",
      btn3: "Trust & Safety",
    },
  },
  faqs: {
    title: "FAQ",
    text: "Frequently asked questions",
    qs1: {
      title: "1. What is HPVideo?",
      text: 'HPVideo is a decentralized AI text-to-video platform deployed on BNB Chain and designed as a future “video infrastructure layer” for the AI Agent era. Users simply connect a wallet, select a video model (such as SORA, WAN, KLING), enter a prompt and duration, pay a small amount of USDT, and generate a high-quality video—without email registration, real-name verification, or any personal information.'
    },
    qs2: {
      title: "2. How does HPVideo generate AI videos?",
      text: 'HPVideo is a decentralized AI text-to-video platform deployed on BNB Chain and designed as a future “video infrastructure layer” for the AI Agent era. Users simply connect a wallet, select a video model (such as SORA, WAN, KLING), enter a prompt and duration, pay a small amount of USDT, and generate a high-quality video—without email registration, real-name verification, or any personal information.'
    },
    qs3: {
      title: "3. Why does HPVideo use wallet login?",
      text: 'HPVideo does not require an email or centralized account. Wallet-based access protects user privacy, increases security, and makes onboarding easy for both Web2 and Web3 users.'
    },
    qs4: {
      title: "4. How is HPVideo decentralized?",
      text: 'Video requests, payments, and timestamps are recorded on BNB Chain, and the generation process is supported by distributed computing. This ensures transparency, verifiability, and trustworthy content ownership.'
    },
    qs5: {
      title: "5. Which AI models does HPVideo support?",
      text: 'HPVideo supports WAN 2.5, SORA 2, OVI, VEO 3.1, LTX 2 Pro, KLING V2.0, PIXVERSE V4.5 and others. Each model specializes in different visual styles and creative scenarios.'
    },
    qs6: {
      title: "6. How much does it cost to generate a video?",
      text: 'Costs depend on the model and duration. Most videos cost between 0.3–1 USDT and are paid through the user’s BNB Chain wallet.'
    },
    qs7: {
      title: "7. Do users own the videos they generate?",
      text: 'Yes. All generated videos belong to the user’s wallet address. On-chain timestamps provide verifiable proof of originality and ownership.'
    },
    qs8: {
      title: "8. Can Web2 creators use HPVideo?",
      text: 'Yes. The process is very simple: connect wallet → enter prompt → generate video. No crypto knowledge or account registration is required.'
    },
    qs9: {
      title: "9. Does HPVideo have a token?",
      text: 'Yes. HPVideo will launch the $HPC token for governance, incentives, compute participation, and ecosystem utility. More details will be published in the official whitepaper.'
    },
    qs10: {
      title: "10. What is HPVideo’s long-term vision?",
      text: 'HPVideo aims to become the decentralized video infrastructure layer for global creators, AI Agents, and Web3 applications, offering transparent generation, verifiable ownership, and a multi-model creative ecosystem.'
    },
  },
  footer: {
    title: "Useful Links",
    meau1: "Github",
    meau2: "Miners",
    meau3: "Whitepapers",
    meau4: "FAQs",
    meau5: "$HPC",
    title1: "Get started with HPVideo",
    btn1: "Try HPVideo",
    btn2: "Watch Demo",
    footer_bottom: "2025 HPVideo. All Right Researved"
  }
}